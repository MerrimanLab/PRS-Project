## Running the Gout GWAS in the UK Biobank cohort

We ran a genome-wide association study in the UK Biobank European cohort with gout as the outcome. This was done with a total of 27,287,012 variants (after imputation), and adjusted for age, sex, and the first 40 genetic principal component vectors.

Initially, the full list of variants for each chromosome was used to define regions that contained 100,000 variants (or the remaining variants if the last region of a chromosome). These splits were converted into a series of files (one per chromosome) with the format of "splitnumber chromosomenumber:startcoord-endcoord" for each row (representing each region). The following code was saved as a script called split_chr.sh in /Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474/splits.

```{bash, engine.opts = '-l', eval = F}
# Setting UKBB_DIR variable.
UKBB_DIR=/Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474

# Set i variable to first input (chromosome number).
i=$1

# Split the corresponding mfi file into sets of 100000 rows per file.
split --lines=100000 --suffix-length=5 --numeric-suffixes=1 $UKBB_DIR/ukb_mfi_chr${i}_v3.txt $UKBB_DIR/splits/chr${i}_

# If chr number less than 10, add a 0 before it.
if [[ $i -lt 10 ]]
then
    i2=$(echo "0${i}")
else
    i2=$i
fi

# Summarize the chr, start, and end coordinates of each file then add them all together as rows of an output file with the row number attached.
for FILE in $UKBB_DIR/splits/chr${i}_*
    do echo "${i2}:$(paste -d '-'  <(head -1 $FILE | cut -f3) <(tail -1 $FILE | cut -f3))"
done | awk '{print NR, $0}' > $UKBB_DIR/splits/split_chr${i}.txt

# Remove the temporary split files.
rm $UKBB_DIR/splits/chr${i}_*
```

<br/>

This script was then run using the following code:

```{bash, engine.opts = '-l', eval = F}
# Setting UKBB_DIR variable.
UKBB_DIR=/Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474

# Running split_chr.sh in parallel for each chromosome.
parallel --progress -P $UKBB_DIR/splits/parallel_process.txt --tmp $UKBB_DIR/splits/tmp/ 'bash {1}/splits/split_chr.sh {2}' ::: $UKBB_DIR ::: {1..22} X XY
```

<br/>

Next, we aimed to convert the genotypes from bgen format into vcf.gz for each split of each chromosome as defined above. The following code was saved as a script called extract_convert.sh in /Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474/splits.

```{bash, engine.opts = '-l', eval = F}
# Setting UKBB_DIR variable.
UKBB_DIR=/Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474

# Loading bcftools version 1.10.2.
module load bcftools/bcftools-1.10.2

# Setting the COORD variable as the first argument passed to this script.
COORD=$1

# Setting the SPLIT, RANGE, CHR and CHR_ALT variables based on the COORD variable.
SPLIT=$(echo $COORD | cut -d " " -f1)
RANGE=$(echo $COORD | cut -d " " -f2)
CHR=$(echo $RANGE | cut -d ":" -f1 | sed "s/^0//g")
CHR_ALT=$(echo $RANGE | cut -d ":" -f1)

# Converting UK Biobank bgen files for chromosomes 1 to 22 into vcf.gz files. This script works on a single chromosome and splits the bgen file into multiple vcf.gz files. The input coordinates are in the form "splitnumber chromosomenumber:startcoord-endcoord".
bgenix -g $UKBB_DIR/ukb_imp_chr${CHR}_v3.bgen \
-i $UKBB_DIR/ukb_imp_chr${CHR}_v3.bgen.bgi -vcf -incl-range $RANGE | \
sed "s/^0//g" | \
bcftools reheader -s $UKBB_DIR/bgen_to_vcf/id_convert.txt  | \
bcftools reheader -h $UKBB_DIR/splits/new_header.txt | \
bcftools view -S $UKBB_DIR/splits/gwas_keep_ids.txt -O z -o $UKBB_DIR/splits/bgenix_convert_chr${CHR_ALT}_${SPLIT}.vcf.gz
```

<br/>

Given that the X chromosome had to be analyzed in a different manner, the following code only applies to the X chromosome splits. This was saved as extract_convert_chrX.sh in /Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474/splits.

```{bash, engine.opts = '-l', eval = F}
# Setting UKBB_DIR variable.
UKBB_DIR=/Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474

# Loading bcftools version 1.10.2.
module load bcftools/bcftools-1.10.2

# Setting the COORD variable as the first argument passed to this script.
COORD=$1

# Setting the SPLIT, RANGE, CHR and CHR_ALT variables based on the COORD variable.
SPLIT=$(echo $COORD | cut -d " " -f1)
RANGE=$(echo $COORD | cut -d " " -f2)
CHR=$(echo $RANGE | cut -d ":" -f1 | sed "s/^0//g")
CHR_ALT=$(echo $RANGE | cut -d ":" -f1)

# Converting UK Biobank bgen files for chromosome X into vcf.gz files. This script works on a single chromosome and splits the bgen file into multiple vcf.gz files. The input coordinates are in the form "splitnumber chromosomenumber:startcoord-endcoord".
bgenix -g $UKBB_DIR/ukb_imp_chr${CHR}_v3.bgen \
-i $UKBB_DIR/ukb_imp_chr${CHR}_v3.bgen.bgi -vcf -incl-range $RANGE | \
sed "s/^0//g" | \
bcftools reheader -s $UKBB_DIR/bgen_to_vcf/id_convert_chrX.txt | \
bcftools reheader -h $UKBB_DIR/splits/new_header_chrX.txt | \
bcftools view -S $UKBB_DIR/splits/gwas_keep_ids.txt --force-samples -O z -o $UKBB_DIR/splits/bgenix_convert_chr${CHR_ALT}_${SPLIT}.vcf.gz
```

<br/>

Finally the XY chromosome had to be analyzed in a different manner, thus the following code only applies to the XY chromosome splits. This was saved as extract_convert_chrXY.sh in /Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474/splits.

```{bash, engine.opts = '-l', eval = F}
# Setting UKBB_DIR variable.
UKBB_DIR=/Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474

# Loading bcftools version 1.10.2.
module load bcftools/bcftools-1.10.2

# Setting the COORD variable as the first argument passed to this script.
COORD=$1

# Setting the SPLIT, RANGE, CHR and CHR_ALT variables based on the COORD variable.
SPLIT=$(echo $COORD | cut -d " " -f1)
RANGE=$(echo $COORD | cut -d " " -f2)
CHR=$(echo $RANGE | cut -d ":" -f1 | sed "s/^0//g")
CHR_ALT=$(echo $RANGE | cut -d ":" -f1)

# Converting UK Biobank bgen files for chromosome XY into vcf.gz files. This script works on a single chromosome and splits the bgen file into multiple vcf.gz files. The input coordinates are in the form "splitnumber chromosomenumber:startcoord-endcoord".
bgenix -g $UKBB_DIR/ukb_imp_chr${CHR}_v3.bgen \
-i $UKBB_DIR/ukb_imp_chr${CHR}_v3.bgen.bgi -vcf -incl-range $RANGE | \
sed "s/^0//g" | \
bcftools reheader -s $UKBB_DIR/bgen_to_vcf/id_convert_chrXY.txt | \
bcftools reheader -h $UKBB_DIR/splits/new_header_chrXY.txt | \
bcftools view -S $UKBB_DIR/splits/gwas_keep_ids.txt --force-samples -O z -o $UKBB_DIR/splits/bgenix_convert_chr${CHR_ALT}_${SPLIT}.vcf.gz
```

<br/>

The above scripts were run in parallel to produce vcf.gz files for each split of each chromosome using the following code.

```{bash, engine.opts = '-l', eval = F}
# Setting UKBB_DIR variable.
UKBB_DIR=/Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474

# Running extract_convert.sh in parallel for each split of each autosome.
parallel --progress -P $UKBB_DIR/splits/parallel_process.txt --tmp $UKBB_DIR/splits/tmp/ 'bash {1}/splits/extract_convert.sh {2}' ::: $UKBB_DIR :::: $UKBB_DIR/splits/split_chr{1..22}.txt

# Running extract_convert_chrX.sh in parallel for each split of the X chromosome.
parallel --progress -P $UKBB_DIR/splits/parallel_process.txt --tmp $UKBB_DIR/splits/tmp/ 'bash {1}/splits/extract_convert_chrX.sh {2}' ::: $UKBB_DIR :::: $UKBB_DIR/splits/split_chrX.txt

# Running extract_convert_chrXY.sh in parallel for each split of the XY chromosome.
parallel --progress -P $UKBB_DIR/splits/parallel_process.txt --tmp $UKBB_DIR/splits/tmp/ 'bash {1}/splits/extract_convert_chrXY.sh {2}' ::: $UKBB_DIR :::: $UKBB_DIR/splits/split_chrXY.txt
```

<br/>

Next, the following code was saved as do_gout_gwas.sh in /Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474/splits.

```{bash, engine.opts = '-l', eval = F}
# Setting UKBB_DIR variable.
UKBB_DIR=/Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474

# Setting the "file" variable based on the first argument passed to the script.
file=$1

# Setting the "mem" variable to 36000, representing 36 Gb of RAM in plink code.
mem=36000

# Converting the file from  the vcf.gz format to the plink binary format "bed/bim/fam".
plink1.9b6.10 --vcf $UKBB_DIR/splits/${file}.vcf.gz --make-bed --out $UKBB_DIR/splits/${file} --memory $mem

# Creating temporary plink fileset which is filtered to keep only individuals with IDs in the gout_gwas_keep_ids_w_sex.txt file, then adding the phenotype information based on the plink_goutaff column of gout_gwas_covar.covar, finally updating the sex variable based on the gout_gwas_keep_ids_w_sex.txt file.
plink1.9b6.10 --bfile $UKBB_DIR/splits/${file} --out $UKBB_DIR/splits/${file}_tmp --keep $UKBB_DIR/splits/gout_gwas_keep_ids_w_sex.txt --pheno $UKBB_DIR/splits/gout_gwas_covar.covar  --pheno-name plink_goutaff --update-sex $UKBB_DIR/splits/gout_gwas_keep_ids_w_sex.txt --make-bed --memory $mem

# Running the logistic regression GWAS, additionally outputting a minor allele frequency report, filtering for SNPs with less than 10% missingness, producing a report of missingness, adding 95% confidence intervals to the logistic regression results, filtering variants to exclude those with minor allele frequency of less than 0.0001 (0.01%), filtering to exclude variants that fail the hardy weinberg equilibrium test with p < 0.000001, producing a hardy weinberg report, then providing the covariates (age and the first 40 principal components) from the gout_gwas_covar.covar file.
plink1.9b6.10 --bfile $UKBB_DIR/splits/${file}_tmp --logistic sex --freq case-control --geno 0.1 --missing --ci 0.95 --maf 0.0001 --hwe 0.000001 --hardy --out $UKBB_DIR/splits/gout_gwas/${file} --covar $UKBB_DIR/splits/gout_gwas_covar.covar --covar-name Age,pc1-pc40 --memory $mem

# Deleting all created plink binary files.
rm $UKBB_DIR/splits/${file}.{bed,bim,fam} $UKBB_DIR/splits/${file}_tmp.{bed,bim,fam}

# Modifying each of the plink reports from running the gwas function to first convert multiple spaces into a single space, then convert spaces to tabs, then remove leading tabs from each row, then remove trailing tabs from each row, then compress the result and save it as a .tsv.gz file, then also remove the original file.
tr -s ' ' < $UKBB_DIR/splits/gout_gwas/${file}.assoc.logistic | tr ' ' '\t' | sed 's/^\t//g' | sed 's/\t$//g' | gzip -c > $UKBB_DIR/splits/gout_gwas/${file}.assoc.logistic.tsv.gz && rm $UKBB_DIR/splits/gout_gwas/${file}.assoc.logistic

tr -s ' ' < $UKBB_DIR/splits/gout_gwas/${file}.frq.cc  | tr ' ' '\t' | sed 's/^\t//g' | sed 's/\t$//g' | gzip -c > $UKBB_DIR/splits/gout_gwas/${file}.frq.cc.tsv.gz && rm $UKBB_DIR/splits/gout_gwas/${file}.frq.cc

tr -s ' ' < $UKBB_DIR/splits/gout_gwas/${file}.hwe | tr ' ' '\t' | sed 's/^\t//g' | sed 's/\t$//g' | gzip -c > $UKBB_DIR/splits/gout_gwas/${file}.hwe.tsv.gz && rm $UKBB_DIR/splits/gout_gwas/${file}.hwe

tr -s ' ' < $UKBB_DIR/splits/gout_gwas/${file}.imiss | tr ' ' '\t' | sed 's/^\t//g' | sed 's/\t$//g' | gzip -c > $UKBB_DIR/splits/gout_gwas/${file}.imiss.tsv.gz && rm $UKBB_DIR/splits/gout_gwas/${file}.imiss

tr -s ' ' < $UKBB_DIR/splits/gout_gwas/${file}.lmiss | tr ' ' '\t' | sed 's/^\t//g' | sed 's/\t$//g' | gzip -c > $UKBB_DIR/splits/gout_gwas/${file}.lmiss.tsv.gz && rm $UKBB_DIR/splits/gout_gwas/${file}.lmiss
```

<br/>

The above script was run in parallel to run a GWAS for gout for each split of each chromosome using the following code.

```{bash, engine.opts = '-l', eval = F}
# Setting UKBB_DIR variable.
UKBB_DIR=/Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474

# Run the command 'bash /Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474/splits/do_gout_gwas.sh {2}' in parallel, each time replacing the {2} with the basename of a .vcf.gz file in the /Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474/splits directory. This command will also print the progress of each task, and will run up to 5 jobs in parallel at a time (this number is stored in parallel_process.txt), also send temporary outputs into a subdirectory called tmp/.
parallel --progress -P $UKBB_DIR/splits/parallel_process.txt --tmp $UKBB_DIR/splits/tmp/ 'bash {1}/splits/do_gout_gwas.sh {2}' ::: $UKBB_DIR ::: $(basename -s .vcf.gz -a $(ls $UKBB_DIR/splits/bgenix_convert_chr*.gz))
```

<br/> 

The gout GWAS output files (*.assoc.logistic.tsv.gz) were then filtered and concatenated together for each chromosome. The following code was saved as concat_gout_gwas.sh in /Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474/splits

```{bash, engine.opts = '-l', eval = F}
# Setting UKBB_DIR variable.
UKBB_DIR=/Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474

# Setting the "i" variable based on the first argument passed to the script.
i=$1

# Concatenating all files together for each chromosome and filtering for SNP effects only (ADD) then sorting numerically by BP position.
for FILE in $UKBB_DIR/splits/gout_gwas/bgenix_convert_chr${i}_*.assoc.logistic.tsv.gz
    do zcat $FILE | awk -F '\t' '$5 == "ADD"'
done | sort -nk 3 > $UKBB_DIR/splits/gout_gwas/gout_gwas_chr${i}_add_unfiltered_p.tsv
```

<br/>

The above script was run in parallel for each chromosome using the following code.

```{bash, engine.opts = '-l', eval = F}
# Setting UKBB_DIR variable.
UKBB_DIR=/Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474

# Run the command 'bash /Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474/splits/concat_gout_gwas.sh {2}' in parallel, each time replacing the {2} with the chromosome number. This command will also print the progress of each task, and will run up to 5 jobs in parallel at a time, also send temporary outputs into a subdirectory called tmp/.
parallel --progress -P $UKBB_DIR/splits/parallel_process.txt --tmp $UKBB_DIR/splits/tmp/ 'bash {1}/splits/concat_gout_gwas.sh {2}' ::: $UKBB_DIR ::: {1..22} X XY
```

<br/> 

Finally, these chromosome wide summary stats were concatenated into ukbb_gout-allcontrol_chr1-22.X.XY.add_unfiltered_p.tsv which was copied to the Data/GWAS directory for our study.

```{bash, engine.opts = '-l', eval = F}
# Setting UKBB_DIR variable.
UKBB_DIR=/Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474

# Making empty file with header row.
zcat $UKBB_DIR/splits/gout_gwas/bgenix_convert_chr10_1.assoc.logistic.tsv.gz | head -1 > $UKBB_DIR/splits/gout_gwas/ukbb_gout-allcontrol_chr1-22.X.XY.add_unfiltered_p.tsv

# Concatenating all summary stats files together.
for file in $UKBB_DIR/splits/gout_gwas/gout_gwas_chr*_add_unfiltered_p.tsv
    do cat $file >> $UKBB_DIR/splits/gout_gwas/ukbb_gout-allcontrol_chr1-22.X.XY.add_unfiltered_p.tsv
done

# Copying file to Data/GWAS directory for this study.
cp $UKBB_DIR/splits/gout_gwas/ukbb_gout-allcontrol_chr1-22.X.XY.add_unfiltered_p.tsv /Volumes/archive/userdata/student_users/nicksumpter/Documents/PhD/PRS/Data/GWAS
```

<br/>

## Filtering the gout GWAS summary statistics

Next, I filtered the gout GWAS summary statistics to only include SNPs that were genotyped on the CoreExome genotyping chip. Additionally, I removed any indels, variants with MAF < 0.01, and X/Y chromosome SNPs, and ensured that all variants were biallelic.

To determine which SNPs were genotyped in the CoreExome, I first had to remove poorly genotyped variants (greater than 5% missingness). This was done by first extracting the IDs of the individuals to be studied in the CoreExome, as these IDs would be needed for accurately removing poorly genotyped variants.

```{r, eval = F}
# Loading phenotype file for CoreExome data.
CoreExPheno <- read_delim(here("Data/Phenotypes/CZ-MB1.2-QC1.10_MergedPhenotypes_20082020.txt"), delim = "\t")

# Loading IDs of all genotyped individuals for the CoreExome.
All_CoreEx_ID <- read_delim("/Volumes/archive/merrimanlab/raid_backup/New_Zealand_Chip_data/CoreExome/QC_MergedBatches/Final_Data/CZ-MB1.2-QC1.10_CoreExome24-1.0-3_genotyped-QCd_rsIDconverted.fam", delim = " ", col_names = F)

# Extracting data on all European IDs from the CoreExome that were genotyped and that were part of the cohorts that we plan on studying, also removing those samples missing gout status.
CoreExPheno_Euro <- CoreExPheno %>% 
  filter(Geno.BroadAncestry == "European",
         Geno.SampleID %in% All_CoreEx_ID$X2,
         General.Use != "No",
         !Pheno.Study %in% c("Auckland Controls", "Australian Controls", "ESR", "Rheumatoid Arthritis"),
         !is.na(Pheno.GoutSummary))

# Extracting data on all Oceanian IDs from the CoreExome that were genotyped and that were part of the cohorts that we plan on studying, also removing those samples missing gout status.
CoreExPheno_Poly <- CoreExPheno %>% 
  filter(Geno.BroadAncestry == "Oceanian",
         Geno.SampleID %in% All_CoreEx_ID$X2,
         General.Use != "No",
         !Pheno.Study %in% c("ESR", "Pacific Trust"),
         !is.na(Pheno.GoutSummary))

# Combining European and Polynesian phenotype files together then extracting FID and IID columns.
all_coreex_ids <- rbind(CoreExPheno_Euro, CoreExPheno_Poly) %>% 
  select(Geno.FamilyID, Geno.SampleID)

# Writing out FID and IID columns as a txt file for filtering.
write_delim(all_coreex_ids, delim = "\t", file = here("Output/Temp/all_coreex_ids.txt"), col_names = F)

# Cleaning up environment.
rm(CoreExPheno, CoreExPheno_Euro, CoreExPheno_Poly, all_coreex_ids, All_CoreEx_ID)
```

<br/>

Next, we used plink to filter the CoreExome genotype file, keeping the list of IDs that we generated above. We then use these IDs to filter out genotypes that are missing in more than 5% of individuals.

```{bash, engine.opts = '-l', eval = F}
# Setting directory as a variable.
PRS_DIR=/Volumes/archive/userdata/student_users/nicksumpter/Documents/PhD/PRS

# Filtering CoreExome plink files to only keep IDs in all_coreex_ids.txt, then filtering out genotypes with more than 5% missingness.
plink1.9b4.9 --bfile /Volumes/archive/merrimanlab/raid_backup/New_Zealand_Chip_data/CoreExome/QC_MergedBatches/Final_Data/CZ-MB1.2-QC1.10_CoreExome24-1.0-3_genotyped-QCd_rsIDconverted \
--keep $PRS_DIR/Output/Temp/all_coreex_ids.txt \
--geno 0.05 \
--make-bed --out $PRS_DIR/Output/Temp/inCoreExGeno
```

<br/>

The resulting list of variants was then used to filter the gout GWAS summary statistics prior to reading the summary statistics file into R.

```{bash, engine.opts = '-l', eval = F}
# Setting directory as a variable.
PRS_DIR=/Volumes/archive/userdata/student_users/nicksumpter/Documents/PhD/PRS

# Reordering the summary statistics columns to put chr and bp next to each other (and removing useless columns).
cat $PRS_DIR/Data/GWAS/ukbb_gout-allcontrol_chr1-22.X.XY.add_unfiltered_p.tsv | awk -v OFS="\t" '{print $1, $3, $2, $4, $7, $8, $9, $10, $11, $12}' > $PRS_DIR/Data/GWAS/ukbb_gout_sumstat.tsv

# Reordering CoreExome bim file for filtering.
cat $PRS_DIR/Output/Temp/inCoreExGeno.bim | awk -v OFS="\t" '{print "^" $1, $4}' > $PRS_DIR/Output/Temp/for_filtering.txt

# Filtering the summary statistics to only include variants that were genotyped in the CoreExome file (using chromosome and BP location).
cat $PRS_DIR/Data/GWAS/ukbb_gout_sumstat.tsv | grep -Ff $PRS_DIR/Output/Temp/for_filtering.txt > $PRS_DIR/Output/Temp/filtered_ukbb_gout_sumstat.tsv
```

<br/>

```{r, eval = F}
# Reading list of filtered variants into R.
# geno <- read_delim(here("Output/Temp", "inCoreExGeno.bim"), delim = "\t", col_names = FALSE) %>% 
#   mutate(CHR_BP = paste0(X1, "_", X4))
# 
# # Reading in summary statistics for gout GWAS.
# sumstat <- vroom(here("Data/GWAS/ukbb_gout-allcontrol_chr1-22.X.XY.add_unfiltered_p.tsv"), 
#                  delim = "\t", 
#                  col_names = TRUE)

# Reading in filtered summary statistics for gout GWAS.
sumstat <- vroom(here("Output/Temp/filtered_ukbb_gout_sumstat.tsv"), 
                 delim = "\t", 
                 col_names = FALSE) %>% 
  rename(CHR = X1,
         BP = X2,
         SNP = X3,
         A1 = X4,
         OR = X5,
         SE = X6,
         L95 = X7,
         U95 = X8,
         STAT = X9,
         P = X10)

# Performing initial filtering of summary statistics to keep only autosomal variants that remained after filtering for high quality CoreExome genotypes. Additionally, some early filtering for indels was performed by keeping only alleles that had length of 1. Finally cleaning up the SNP column by separating into 2 columns using the comma delimiter. This resulted in 324,560 (previously 307,368) remaining variants.
sumstat2 <- sumstat %>% 
  filter(CHR %in% 1:22,
         str_length(A1) == 1) %>% 
  separate(SNP, into = c("SNP1", "SNP2", "SNP3"), sep = ",", remove = FALSE)

# Checking what third column contained (just chromosomes, so can remove).
table(sumstat2$SNP3)

# Removing third SNP column.
sumstat2 <- sumstat2 %>% 
  select(-SNP3)

# Extracting 323,467 (previously 307,066) variants with an rsID in the SNP1 column.
rsid_col1 <- sumstat2 %>% 
  filter(str_detect(SNP1, regex("^rs[0-9]+")))

# Extracting 75,332 (previously 74,955) variants with an rsID in the SNP1 and SNP2 columns.
rsid_col1_col2 <- rsid_col1 %>%
  filter(str_detect(SNP2, regex("^rs[0-9]+")))

# Extracting 63 variants with two different rsIDs, for the most part SNP1 appears to be the newest rsID, but I will keep the extra rsID in a separate column.
different_rsids <- rsid_col1_col2 %>%
  filter(SNP1 != SNP2)

# Extracting 247,189 (previously 231,166) variants with an rsID in the SNP1 column and a SNP name of the format chr:bp_a1_a2 in the SNP2 column.
rsid_col1_other_col2 <- rsid_col1 %>% 
  filter(str_detect(SNP2, regex("^[0-9]+:[0-9]+_[ACGT]+_[ACGT]+$")))

# Splitting SNP2 column into various parts.
rsid_col1_other_col2 <- rsid_col1_other_col2 %>% 
  mutate(CHR2 = SNP2 %>% str_split(":", simplify = TRUE) %>% .[,1] %>% as.numeric(),
         BP2 = SNP2 %>% str_split(":", simplify = TRUE) %>% .[,2] %>% str_split("_", simplify = TRUE) %>% .[,1] %>% as.numeric(),
         Allele1 = SNP2 %>% str_split("_", simplify = TRUE) %>% .[,2],
         Allele2 = SNP2 %>% str_split("_", simplify = TRUE) %>% .[,3])

# Showing that the BP and BP2 columns are equal.
sum(rsid_col1_other_col2$BP != rsid_col1_other_col2$BP2)

# Showing that the CHR and CHR2 columns are equal.
sum(rsid_col1_other_col2$CHR != rsid_col1_other_col2$CHR2)

# Testing whether the A1 and Allele2 columns are equal shows that they are not always equal.
sum(rsid_col1_other_col2$A1 != rsid_col1_other_col2$Allele2) # Allele2 is not always A1

# Removing CHR2 and BP2 columns then further filtering out indels based on Allele1 and Allele2 columns. This removes a further 175 (previously 13) indels resulting in 247,014 (previously 231,153) variants.
rsid_col1_other_col2 <- rsid_col1_other_col2 %>% 
  select(-CHR2, -BP2) %>% 
  filter(str_length(Allele1) == 1,
         str_length(Allele2) == 1)

# Isolating SNP, Allele1 and Allele2 columns.
rsid_col1_other_col2 <- rsid_col1_other_col2 %>% 
  select(SNP, Allele1, Allele2)

# Filtering to keep 945 variants with neither rsID nor chr:bp_a1_a2 in the SNP2 column.
rsid_col1_unknown_col2 <- rsid_col1 %>%
  filter(!str_detect(SNP2, regex("^[0-9]+:[0-9]+_[ACGT]+_[ACGT]+$|^rs[0-9]+$")))

# Showing that all of these remaining SNPs are in the format Affx-<number>.
sum(!str_detect(rsid_col1_unknown_col2$SNP2, regex("^Affx-[0-9]+$"))) # 

# Making clean final list of variants from those with an rsID in the SNP1 column.
rsid_col1_final <- rsid_col1 %>% 
  mutate(RSID = SNP1,
         ALT_RSID = case_when(str_detect(SNP2, regex("^rs[0-9]+$")) & SNP1 != SNP2 ~ SNP2, TRUE ~ NA_character_),
         AFFYID = case_when(str_detect(SNP2, regex("^Affx-[0-9]+$")) ~ SNP2, TRUE ~ NA_character_),
         SNP_ID = case_when(str_detect(SNP2, regex("^[0-9]+:[0-9]+_[ACGT]+_[ACGT]+$")) ~ SNP2, TRUE ~ NA_character_)) %>% 
  left_join(rsid_col1_other_col2, by = "SNP") %>% 
  filter(is.na(SNP_ID) | (!is.na(SNP_ID) & !is.na(Allele1))) # removing the indels

# Extracting variants without an rsID in the SNP1 column. 1,093 (previously 302 variants) don't have an rsID in SNP1 column.
not_rsid_col1 <- sumstat2 %>% 
  filter(!str_detect(SNP1, regex("^rs[0-9]+")))

# Extracting variants with snp_id format in the SNP1 column. 1,091 (previously 300) have the snp_id format in SNP1.
snp_id_col1 <- not_rsid_col1 %>% 
  filter(str_detect(SNP1, regex("^[0-9]+:[0-9]+_[ACGT]+_[ACGT]+$")))

# Splitting up the snp_id column.
snp_id_col1 <- snp_id_col1 %>% 
  mutate(CHR2 = SNP1 %>% str_split(":", simplify = TRUE) %>% .[,1] %>% as.numeric(),
         BP2 = SNP1 %>% str_split(":", simplify = TRUE) %>% .[,2] %>% str_split("_", simplify = TRUE) %>% .[,1] %>% as.numeric(),
         Allele1 = SNP1 %>% str_split("_", simplify = TRUE) %>% .[,2],
         Allele2 = SNP1 %>% str_split("_", simplify = TRUE) %>% .[,3])

# Showing that the BP and BP2 columns are equal.
sum(snp_id_col1$BP != snp_id_col1$BP2)

# Showing that the CHR and CHR2 columns are equal.
sum(snp_id_col1$CHR != snp_id_col1$CHR2)

# Testing whether the A1 and Allele2 columns are equal shows that they are not always equal.
sum(snp_id_col1$A1 != snp_id_col1$Allele2)

# Removing indels (1,091 down to 482 remaining (288 previously) - 609 removed (previously 12))
snp_id_col1 <- snp_id_col1 %>% 
  select(-CHR2, -BP2) %>% 
  filter(str_length(Allele1) == 1,
         str_length(Allele2) == 1)

# Isolating SNP, Allele1 and Allele2 columns.
snp_id_col1_final <- snp_id_col1 %>% 
  select(SNP, Allele1, Allele2)

# Isolating 7 of the 482 variants with an rsID in the SNP2 column (previously 7 of 300).
snp_id_col1_rsid_col2 <- snp_id_col1 %>%
  filter(str_detect(SNP2, regex("^rs[0-9]+")))

# Isolating 474 of the 482 variants with a snp_id in the SNP2 column (previously 292 of 300).
snp_id_col1_snp_id_col2 <- snp_id_col1 %>%
  filter(str_detect(SNP2, regex("^[0-9]+:[0-9]+_[ACGT]+_[ACGT]+$")))

# Showing that all of these variants have identical SNP1 and SNP2 columns.
sum(snp_id_col1_snp_id_col2$SNP1 != snp_id_col1_snp_id_col2$SNP2)

# Remaining SNP has an affy ID in SNP2 column.
snp_id_col1_affy_col2 <- snp_id_col1 %>%
  filter(str_detect(SNP2, regex("^Affx-[0-9]+$")))

# Two total SNPs have an affy ID in the SNP1 column.
affy_col1 <- not_rsid_col1 %>%
  filter(str_detect(SNP1, regex("^Affx-[0-9]+$")))

# Both have an affy ID in the SNP2 column.
affy_col1_affy_col2 <- affy_col1 %>%
  filter(str_detect(SNP2, regex("^Affx-[0-9]+$")))

# Showing that both of these variants have identical SNP1 and SNP2 columns.
sum(affy_col1_affy_col2$SNP1 != affy_col1_affy_col2$SNP2)

# Creating final list of SNPs without an rsID in column 1.
not_rsid_col1_final <- not_rsid_col1 %>% 
  mutate(RSID = case_when(str_detect(SNP2, regex("^rs[0-9]+$")) ~ SNP2, TRUE ~ NA_character_),
         ALT_RSID = NA_character_,
         AFFYID = case_when(str_detect(SNP2, regex("^Affx-[0-9]+$")) ~ SNP2, TRUE ~ NA_character_),
         SNP_ID = case_when(str_detect(SNP1, regex("^[0-9]+:[0-9]+_[ACGT]+_[ACGT]+$")) ~ SNP1, TRUE ~ NA_character_)) %>% 
  left_join(snp_id_col1_final, by = "SNP") %>% 
  filter(is.na(SNP_ID) | (!is.na(SNP_ID) & !is.na(Allele1))) # removing the indels

# Combining two SNP lists back together resulting in 323,776 variants (previously 307,343)
sumstat3 <- rbind(rsid_col1_final, not_rsid_col1_final) %>% 
  arrange(CHR, BP)

# Cleaning up the environment.
remove <- ls()
remove <- as_tibble(remove) %>%
  filter(str_detect(value, "col"))
remove <- remove$value
rm(list = remove, remove)
rm(different_rsids)

# 901 duplicates need to be removed.
tmp <- sumstat3 %>%
 select(CHR, BP) %>%
 unique()

# Writing out list of locations for each chromosome.
for(i in 1:22) {
  write_delim(select(filter(tmp, CHR == i), BP), file = paste0(here("Output/Temp/"), "chr", i, "_snplist.txt"), delim = "\n")
}
```

<br/>

```{bash, engine.opts = '-l', eval = F}
# Setting directory as a variable.
PRS_DIR=/Volumes/archive/userdata/student_users/nicksumpter/Documents/PhD/PRS

# Now to pull out the MAF and alleles for all SNPs using the mfi files (based on the SNP location files we just created).
parallel "grep -Fwhf {1}/Output/Temp/chr{2}_snplist.txt /Volumes/scratch/merrimanlab/ukbio/EGAD00010001474/splits/ukb_mfi_chr{2}_v3.txt > {1}/Output/Temp/ukb_maf_info_chr{2}.txt" ::: ${PRS_DIR} ::: {1..22}'
```

<br/> 

```{r, eval = F}
# Reading back in the filtered mfi files and combining them together (total of 319,376 variants).
out <- tibble()
for(i in 1:22) {
  assign(paste0("chr", i, "_snps"), read_delim(here("Output/Temp", paste0("ukb_maf_info_chr", i, ".txt")), delim = "\t", col_names = FALSE) %>% mutate(CHR = i))
  out <- rbind(out, get(paste0("chr", i, "_snps")))
  rm(list = paste0("chr", i, "_snps"), i)
}

# Updating the column names of this file.
colnames(out) <- c("SNP1_mfi", "SNP2_mfi", "BP_mfi", "Allele1_mfi", "Allele2_mfi", "MAF_mfi", "Minor_Allele_mfi", "INFO_mfi", "CHR_mfi")

# Showing that all have allele1 and allele2.
sum(is.na(out$Allele1_mfi) | is.na(out$Allele2_mfi))

# Removing any remaining indels, any variants with less than 1% allele frequency in the UK Biobank, and any variants with less than 0.3 INFO scores (indicating poor imputation quality). This results in 246,869 variants.
out <- out %>% 
  mutate(CHR_BP = paste0(CHR_mfi, "_", BP_mfi)) %>% 
  filter(Allele1_mfi %in% c("A", "C", "G", "T"),
         Allele2_mfi %in% c("A", "C", "G", "T"),
         MAF_mfi > 0.01,
         MAF_mfi < 0.99,
         INFO_mfi > 0.3)

# 224 of these are multi-allelic sites.
sum(duplicated(out$CHR_BP))

# Adding information from above and filtering out missing variants. Results in 246,017 variants.
sumstat4 <- sumstat3 %>% 
  mutate(CHR_BP = paste0(CHR, "_", BP)) %>% 
  left_join(out, by = "CHR_BP") %>% 
  filter(!is.na(Allele1_mfi)) %>% 
  select(-CHR_mfi, -BP_mfi)

# 823 duplicated sites are in the sumstat4 file.
tmp <- sumstat4 %>% 
  filter(duplicated(CHR_BP)) %>% 
  pull(CHR_BP) %>% 
  unique()

# A total of 244,015 locations have only one SNP according to the mfi files. All remaining variants should be removed from further analysis.
tmp2 <- sumstat4 %>% 
  filter(!(CHR_BP %in% tmp))

# Reading back in genotype file (bim file).
geno <- read_delim(here("Output/Temp", "inCoreExGeno.bim"), delim = "\t", col_names = FALSE) %>% 
  mutate(CHR_BP = paste0(X1, "_", X4))

# Identifying locations that are biallelic in the genotype files (also removing indels for this comparison). Only 243,832 variants remain (183 fewer than tmp2).
geno2 <- geno %>%
  filter(CHR_BP %in% tmp2$CHR_BP,
         X5 %in% c("A", "C", "G", "T"),
         X6 %in% c("A", "C", "G", "T"))

# No further duplicated variants remain.
tmp <- geno2 %>% 
  filter(duplicated(CHR_BP)) %>% 
  pull(CHR_BP) %>% 
  unique()

# Adding SNP ID and genotype information from bim file.
sumstat5 <- tmp2 %>%
  left_join(geno2, by = "CHR_BP") %>%
  select(-X1, -X3, -X4)

# Cleaning up environment.
rm(geno, geno2, out, tmp2, tmp3, tmp4, tmp)

# Identifying which allele columns are identical.
# A1 column has no missing data.
sum(is.na(sumstat5$A1))

# Allele1_mfi column has no missing data.
sum(is.na(sumstat5$Allele1_mfi))

# Allele2_mfi column has no missing data.
sum(is.na(sumstat5$Allele2_mfi))

# Allele1 column is missing in 47,667 rows.
sum(is.na(sumstat5$Allele1))

# Allele2 column is missing in 47,667 rows.
sum(is.na(sumstat5$Allele2))

# X5 column is missing in 183 rows.
sum(is.na(sumstat5$X5))

# X6 column is missing in 183 rows.
sum(is.na(sumstat5$X6))

# Allele1_mfi is identical to Allele1 but has no missing data, thus it should overwrite Allele1.
test <- sumstat5 %>%
  filter(!is.na(Allele1),
         Allele1 != Allele1_mfi)

# Updating columns based on this information.
sumstat5 <- sumstat5 %>% 
  select(-Allele1) %>% 
  rename(Allele1 = Allele1_mfi)

# Allele2_mfi is almost identical to Allele2 excluding missing data, thus it should overwrite Allele2. However, these 18 rows need to be removed as they don't match suggesting multi-allelic sites remain.
test <- sumstat5 %>%
  filter(!is.na(Allele2),
         Allele2 != Allele2_mfi)

# Updating columns based on this information and removing 18 mismatching rows.
sumstat5 <- sumstat5 %>% 
  select(-Allele2) %>% 
  rename(Allele2 = Allele2_mfi) %>% 
  filter(!(CHR_BP %in% test$CHR_BP))

# Some variants don't match the alleles between the mfi files and the genotype files (these should therefore be removed).
test <- sumstat5 %>% 
  filter((Allele1 == X5 | Allele1 == X6) & (Allele2 == X5 | Allele2 == X6))

# Filtering based on above information.
sumstat5 <- sumstat5 %>% 
  filter((Allele1 == X5 | Allele1 == X6) & (Allele2 == X5 | Allele2 == X6))

# All remaining variants match between the various sources of information.
test <- sumstat5 %>% 
  filter(A1 == X5 | A1 == X6)

# SNP1 and SNP2_mfi columns are identical.
sum(sumstat5$SNP1 != sumstat5$SNP2_mfi)

# SNP2 and SNP1_mfi columns are almost identical (48 different).
sum(sumstat5$SNP2 != sumstat5$SNP1_mfi)

# These are just the variants with the chromosome attached to the ID, can safely remove the mfi SNP names.
test <- sumstat5 %>% 
  filter(SNP2 != SNP1_mfi)

# Cleaning up the column names and removing redundant information.
sumstat5 <- sumstat5 %>% 
  rename(Effect_Allele = A1,
         INFO = INFO_mfi,
         MAF = MAF_mfi,
         Minor_Allele = Minor_Allele_mfi,
         BIM_ID = X2) %>% 
  select(-CHR_BP, -X5, -X6, -SNP1_mfi, -SNP2_mfi)

# In 1,419 cases that minor allele column doesn't match the effect allele.
test <- sumstat5 %>% 
  filter(Minor_Allele != Effect_Allele)

# All really close to 0.5 MAF, just need to flip OR, L95, and U95 then set Effect_Allele to Minor_Allele column
summary(test$MAF) 

# Flipping allele order etc. for these variants.
test <- test %>% 
  mutate(OR = 1/OR,
         tmp = 1/L95,
         tmp2 = 1/U95,
         L95 = tmp2,
         U95 = tmp,
         Effect_Allele = Minor_Allele) %>% 
  rename(EAF = MAF) %>% 
  select(-tmp, -tmp2, -Minor_Allele)

# Joining back together with remaining variants after cleaning up.
sumstat5_1 <- sumstat5 %>% 
  filter(Minor_Allele == Effect_Allele) %>% 
  select(-Minor_Allele) %>% 
  rename(EAF = MAF) %>% 
  rbind(test) %>% 
  arrange(CHR, BP)

# Changing allele columns to be named effect/alternate.
test <- sumstat5_1 %>% 
  filter(Allele2 == Effect_Allele) %>% 
  rename(Alternate_Allele = Allele1) %>% 
  select(CHR, SNP, BP, Effect_Allele, Alternate_Allele, OR:SNP_ID, BIM_ID, EAF:INFO)

# Changing allele columns to be named effect/alternate.
test2 <- sumstat5_1 %>% 
  filter(Allele1 == Effect_Allele) %>% 
  rename(Alternate_Allele = Allele2) %>% 
  select(CHR, SNP, BP, Effect_Allele, Alternate_Allele, OR:SNP_ID, BIM_ID, EAF:INFO)

# Joining back together into final summary statistic file.
sumstat_final <- rbind(test, test2) %>% 
  arrange(CHR, BP)

# Saving final summary statistic file.
save(sumstat_final, file = here("Output/sumstat_final.RData"))

# Cleaning up environment.
rm(sumstat, sumstat2, sumstat3, sumstat4, sumstat5, sumstat5_1, test, test2)
```

<br/>

To get from these cleaned up summary statistics to the final list of SNPs for the PRS, I did the following:

1. I filtered out all SNPs with P-values greater than 5e-8.

2. I took each lead SNP within a 1 Mb window and used these to define 15 crude loci.

3. This list of lead SNPs was further filtered to only include one lead SNP per full locus.

    - The boundaries of these "full loci" were defined based on two consecutive genome-wide significant SNPs being more than 500 kb apart.
    
4. Next, SNPs in the UK Biobank BGEN files were extracted if they fit within the boundaries of these "full loci".

5. Conditional GWAS were run at each locus, conditioning on the lead SNP.

6. If there was a significant SNP (P < 5e-8) remaining after conditioning, the original lead SNP and the new lead SNP were used for a subsequent conditional GWAS at this locus.

7. This was repeated until no more significant SNPs (P < 5e-8) remained at each locus.

8. Locus zooms were plotted for each locus, using both the unconditioned and conditioned GWAS results.

9. Finally, the resulting list of 19 lead SNPs were saved in a single file ready for conversion to a PRS.

```{r, eval = F}
# Defining one SNP per locus ---------------------------------------------------------------------------
# Keeping SNPs with P < 5e-8 SNPs and arranging from smallest P to largest P.
sumstat_signif <- sumstat_final %>% 
  filter(P <= 5e-8) %>% 
  arrange(P)

# Grouping into loci +- 500 kb of top SNPs.
# Extracting first row of file (most significant SNP).
gout_top <- sumstat_signif %>% 
  slice(1)

# Removing SNPs within 500 kb window of this lead SNP.
gout2 <- sumstat_signif %>% 
  filter(!(CHR == gout_top$CHR[1] & BP %in% ((gout_top$BP[1] - 500000):(gout_top$BP[1] + 500000))))

# Continuing this process until no variants remain.
while(nrow(gout2) > 0) {
  tmp <- gout2 %>% 
    slice(1)
  gout_top <- rbind(tmp, gout_top)
  gout2 <- gout2 %>% 
    filter(!(CHR == gout_top$CHR[1] & BP %in% ((gout_top$BP[1] - 500000):(gout_top$BP[1] + 500000))))
} 

# Arranging output list by chromosome and bp.
gout_top <- gout_top %>% 
  arrange(CHR, BP)

# Cleaning up environment.
rm(gout2, tmp)

# Finding regions of loci.
# First arranging significant summary stats by chromosome and bp.
sumstat_signif <- sumstat_signif %>% 
  arrange(CHR, BP)

# Next finding difference in position between each variant.
out <- NA
for(i in 2:nrow(sumstat_signif)) {
  if(sumstat_signif$CHR[i] == sumstat_signif$CHR[i - 1]){
    out[i] <- sumstat_signif$BP[i] - sumstat_signif$BP[i - 1]
  } else {
    out[i] <- NA
  }
}

# Making column for filtering using differences of less than 500,000 bp between variants.
tmp <- sumstat_signif %>% 
  mutate(Diff = out,
         Diff2 = case_when(Diff < 500000 ~ Diff))

# Keeping first and last significant variant of each locus based on differences in location between consecutive variants.
# Keeping first SNP as this marks the start of the first locus.
out <- sumstat_signif %>% slice(1)

# For all other SNPs, if there is a difference of greater than 500 kb with the previous SNP then extract that SNP and the previous SNP (as these define the boundaries of loci).
for(i in 2:nrow(sumstat_signif)) {
  if(is.na(tmp$Diff2[i])){
    out <- rbind(out, sumstat_signif %>% slice(i - 1), sumstat_signif %>% slice(i))
  }
}
out <- rbind(out, sumstat_signif %>% slice(nrow(sumstat_signif)))

# Extracting regions of loci.
# First keeping only chromosome and bp columns.
bgen_ranges <- out %>% select(CHR, BP)

# Next extracting every locus start coordinate.
tmp1 <- bgen_ranges %>% slice(seq(1, nrow(bgen_ranges), by = 2)) %>% rename(BP1 = BP)

# Then extracting every locus end coordinate.
tmp2 <- bgen_ranges %>% slice(seq(2, nrow(bgen_ranges), by = 2)) %>% rename(CHR.x = CHR, BP2 = BP)

# Combining back together and adding 50 kb buffer to the edge of each locus.
bgen_ranges <- tmp1 %>% 
  cbind(tmp2) %>% 
  mutate(BP1 = BP1 - 50000,
         BP2 = BP2 + 50000) %>% 
  select(-CHR.x)

# Creating file for filtering UK Biobank genotypes.
# First for chromosomes less than 10 there needs to be a leading 0 before the chromosome number.
bgen_range1 <- bgen_ranges %>% 
  filter(CHR < 10) %>% 
  mutate(BGEN = paste0("0", CHR, ":", BP1, "-", BP2))

# All remaining chromosomes then get treated similarly.
bgen_range2 <- bgen_ranges %>% 
  filter(CHR > 9) %>% 
  mutate(BGEN = paste0(CHR, ":", BP1, "-", BP2))

# Combining ranges back together.
bgen_ranges <- rbind(bgen_range1, bgen_range2) %>% 
  arrange(CHR, BP1)

# Selecting out only BGEN column for use in extracting SNPs from the UK Biobank cohort.
tmp <- bgen_ranges %>% 
  select(BGEN)

# Cleaning up the environment.
rm(bgen_range1, bgen_range2, tmp1, tmp2, out, i)
  
# Writing out file for filtering UK Biobank SNPs.
write_delim(tmp, file = here("Output/Temp", "bgen_range.txt"), delim = "\n", col_names = F)

# Extracting all SNPs from the summary statistics file that fit within the boundaries of these loci.
loci <- tibble()
for(i in 1:nrow(bgen_ranges)){
  tmp <- sumstat_final %>% 
    filter(CHR == bgen_ranges$CHR[i] & between(BP, bgen_ranges$BP1[i], bgen_ranges$BP2[i]))
  loci <- rbind(loci, tmp)
}

# Adding second SNP_ID column.
loci <- loci %>% 
  mutate(SNP_ID2 = paste0(CHR, "_", BP, "_", Alternate_Allele, "_", Effect_Allele))

# Making file for extracting variants from plink files.
tmp <- loci %>% 
  mutate(BP2 = BP) %>% 
  select(CHR, BP, BP2, SNP)

# Writing out file for extracting variants from plink files.
write_delim(tmp, file = here("Output/Temp", "loci_snps.txt"), delim = "\t", col_names = F)

# Keeping most significant SNP at each locus (one per locus = 15 variants).
out <- c()
for(i in 1:nrow(bgen_ranges)){
  tmp <- gout_top %>% 
    filter(CHR == bgen_ranges$CHR[i] & between(BP, bgen_ranges$BP1[i], bgen_ranges$BP2[i])) %>% 
    arrange(P) %>% 
    slice(1)
  out <- rbind(out, tmp)
}

# Adding bgen_ranges columns.
gout_top <- out %>% 
  cbind(bgen_ranges %>% select(-CHR))

# Cleaning up environment.
rm(tmp, bgen_ranges, out, i)

# Making file with list of unique chromosomes for bash script below.
write_delim(unique(gout_top %>% select(CHR)), file = here("Output/Temp/unique_chr.txt"), col_names = F)
```

<br/>

```{bash, engine.opts = '-l', eval = F}
# Setting directory as a variable.
PRS_DIR=/Volumes/archive/userdata/student_users/nicksumpter/Documents/PhD/PRS
UKBB_DIR=/Volumes/archive/merrimanlab_nobackup/ukbio/EGAD00010001474

# Extracting all SNPs at loci from bgen files and converting to vcf format.
parallel "bgenix -g {2}/ukb_imp_chr{3}_v3.bgen -vcf -incl-range {1}/Output/Temp/bgen_range.txt | bcftools reheader -h {2}/bgen_to_vcf/new_header.txt | bcftools annotate --rename-chrs {2}/bgen_to_vcf/rename_contigs.txt | bgzip -c > {1}/Output/Temp/chr{3}_forclumping.vcf.gz" ::: ${PRS_DIR} ::: ${UKBB_DIR} :::: ${PRS_DIR}/Output/Temp/unique_chr.txt

# Converting from plink to vcf, also adding phenotype data, extracting only variants of interest, removing variants with >10% missingness, removing variants with less than 1% frequency, removing variants that fail hardy weinberg equilibrium test with a P < 0.000001
parallel "plink1.9b4.9 --vcf {1}/Output/Temp/chr{2}_forclumping.vcf.gz --extract range {1}/Output/Temp/loci_snps.txt --pheno {1}/Data/GWAS/gout_gwas_covar.covar --pheno-name plink_goutaff --update-sex {1}/Data/GWAS/gout_gwas_keep_ids_w_sex.txt --geno 0.1 --maf 0.01 --hwe 0.000001 --make-bed --out {1}/Output/Temp/chr{2}_tmp"  ::: ${PRS_DIR} :::: ${PRS_DIR}/Output/Temp/unique_chr.txt
```

<br/>

```{r, eval = F}
# Reading the bim files into R and converting their identifiers to just the rsid.
# First listing all files that end in _tmp.bim.
file_names <- list.files(here("Output/Temp/"))[str_detect(list.files(here("Output/Temp/")), "_tmp.bim")]

# Then reading in each file that matches the above format.
for(i in file_names){
  # Reading in file and saving as the file name.
  assign(i, read_delim(paste0(here("Output/Temp/"), i), delim = "\t", col_names = F))
  
  # Adding information from loci table and making clean SNP ID column.
  assign(i, get(i) %>% left_join(loci, (by = c("X1" = "CHR", "X4" = "BP"))) %>% mutate(SNP_clean = case_when(is.na(RSID) ~ SNP_ID, TRUE ~ RSID)))
  
  # Making new bim file with cleaned up SNP column.
  assign(paste0("new_", i), get(i) %>% select(X1, SNP_clean, X3:X6))
  
  # Overwriting existing bim file with cleaned version.
  write_delim(get(paste0("new_", i)), file = paste0(here("Output/Temp/"), i), delim = "\t", col_names = F)
}

# Cleaning up environment.
rm(list = ls()[str_detect(ls(), ".bim")], i, file_names)



# Running the conditional GWAS. ----------------------------------------
# Splitting up the plink files to have one locus per file (saves on computational time).
# Making files for extracting SNPs from plink files.
gout_top2 <- gout_top %>% 
  select(CHR, BP1, BP2, RSID)

for(i in 1:nrow(gout_top2)){
  tmp <- gout_top2 %>% slice(i)
  write_delim(tmp, file = paste0(here("Output/Temp/"), "extractrange_", tmp$RSID, ".txt"), delim = "\t", col_names = F)
}

# Making file with list of all chromosomes for bash script below.
write_delim(gout_top %>% select(CHR), file = here("Output/Temp/all_chr.txt"), col_names = F)

# Making file with list of all SNPs for bash script below.
write_delim(gout_top %>% select(RSID), file = here("Output/Temp/all_rsid.txt"), col_names = F)

# Printing split numbers for easy copy pasting below.
paste(paste0(0, 0:9), collapse = " ")
```

<br/>

```{bash, engine.opts = '-l', eval = F}
# Setting directory as a variable.
PRS_DIR=/Volumes/archive/userdata/student_users/nicksumpter/Documents/PhD/PRS

# Filtering plink files to keep SNPs of interest for each conditional GWAS.
parallel --xapply "plink1.9b6.10 --bfile {1}/Output/Temp/chr{2}_tmp --extract range {1}/Output/Temp/extractrange_{3}.txt --make-bed --out {1}/Output/Temp/{3}" ::: ${PRS_DIR} :::: ${PRS_DIR}/Output/Temp/all_chr.txt :::: ${PRS_DIR}/Output/Temp/all_rsid.txt

# Cutting out list of SNPs from bim file then splitting the list into 10 parts.
parallel "cat {1}/Output/Temp/{2}.bim | cut -f 2 > {1}/Output/Temp/{2}_snps; split -d -n l/10 {1}/Output/Temp/{2}_snps {1}/Output/Temp/{2}_snps_split" ::: ${PRS_DIR} :::: ${PRS_DIR}/Output/Temp/all_rsid.txt

# Concatenating the conditioning SNP of interest onto each split file.
parallel "echo {2} >> {1}/Output/Temp/{2}_snps_split{3}" ::: ${PRS_DIR} :::: ${PRS_DIR}/Output/Temp/all_rsid.txt ::: 00 01 02 03 04 05 06 07 08 09

# Running conditional GWAS's in parallel.
parallel "plink1.9b6.10 --bfile {1}/Output/Temp/{2} --extract {1}/Output/Temp/{2}_snps_split{3} --logistic sex --ci 0.95 --covar {1}/Data/GWAS/gout_gwas_covar.covar --covar-name Age,pc1-pc40 --condition {2} --out {1}/Output/Temp/{2}_split{3}" ::: ${PRS_DIR} :::: ${PRS_DIR}/Output/Temp/all_rsid.txt ::: 00 01 02 03 04 05 06 07 08 09
```

<br/>

```{r, eval = F}
# Listing file names of all the newly generated GWAS summary statistics.
file_names <- list.files(here("Output/Temp/"))[str_detect(list.files(here("Output/Temp/")), regex("rs[0-9]+_split[0-9]+.assoc.logistic"))]

# Reading all the GWAS summary statistics back into R.
for(i in file_names){
  assign(i, read.table(paste0(here("Output/Temp/"), i), header = T) %>% filter(TEST == "ADD"))
}

# Combining the summary statistics for each split together to give a complete summary stat file for each SNP. Also extracting the new lead SNP and adding a column to the unconditioned summary stats.
# Making temporary empty vector called tmp.
tmp <- c()

# Initiating for loop over each row of the gout_top table (i.e. for each of the 15 loci).
for(i in 1:nrow(gout_top)){
  
  # Making temporary empty vector called tmp3.
  tmp3 <- c()
  
  # Initiating nested for loop over the values from 0 to 9.
  for(j in 0:9){
    
    # Save summary statistic for the current SNP and current split number as tmp2.
    tmp2 <- get(paste0(gout_top$RSID[i], "_split0", j, ".assoc.logistic"))
    
    # Concatenate each summary statistic together into tmp3.
    tmp3 <- rbind(tmp3, tmp2)
  }
  
  # Remove any rows containing NA's (i.e. those conditioned on themselves) and save as <RSID>_gwas.
  assign(paste0(gout_top$RSID[i], "_gwas"), tmp3 %>% na.omit())
  
  # Extract the most significant SNP after conditioning along with its p-value.
  tmp3 <- tmp3 %>% select(SNP, P) %>% arrange(P) %>% slice(1)
  
  # Add the new lead SNP to the tmp table.
  tmp <- rbind(tmp, tmp3)
}

# Renaming columns.
tmp <- tmp %>% 
  rename(new_lead = SNP, new_p = P)

# Adding new lead SNP and P-value columns.
gout_top2 <- gout_top %>% 
  cbind(tmp)

# Filtering to only keep SNPs that are still significant at P < 5e-8.
gout_top_resid <- gout_top2 %>%
  filter(new_p < 5e-8)

# Cleaning up environment.
rm(list = ls()[str_detect(ls(), ".assoc")], i, tmp, tmp2, file_names, tmp3, j)

# Second round of conditioning.
# Making file with list of all remaining SNPs for bash script below.
write_delim(gout_top_resid %>% select(RSID), file = here("Output/Temp/round1_rsid.txt"), col_names = F)

# Making file with list of all new lead SNPs for bash script below.
write_delim(gout_top_resid %>% select(new_lead), file = here("Output/Temp/round1_rsid2.txt"), col_names = F)
```

<br/>

```{bash, eval = F}
# Setting directory as a variable.
PRS_DIR=/Volumes/archive/userdata/student_users/nicksumpter/Documents/PhD/PRS

# Concatenating the conditioning SNP of interest onto each split file.
parallel "echo {4} >> {1}/Output/Temp/{2}_snps_split{3}" ::: ${PRS_DIR} :::: ${PRS_DIR}/Output/Temp/round1_rsid.txt ::: 00 01 02 03 04 05 06 07 08 09 :::: ${PRS_DIR}/Output/Temp/round1_rsid2.txt

# Writing out SNP IDs into a series of files for conditioning next round of GWAS.
parallel --xapply "echo $'{2}\n{3}' > {1}/Output/Temp/{2}_2" ::: ${PRS_DIR} :::: ${PRS_DIR}/Output/Temp/round1_rsid.txt :::: ${PRS_DIR}/Output/Temp/round1_rsid2.txt

# Running second round of conditional GWAS's in parallel.
parallel "plink1.9b6.10 --bfile {1}/Output/Temp/{2} --extract {1}/Output/Temp/{2}_snps_split{3} --logistic sex --ci 0.95 --covar {1}/Data/GWAS/gout_gwas_covar.covar --covar-name Age,pc1-pc40 --condition-list {1}/Output/Temp/{2}_2 --out {1}/Output/Temp/{2}_split{3}_2" ::: ${PRS_DIR} :::: ${PRS_DIR}/Output/Temp/round1_rsid.txt ::: 00 01 02 03 04 05 06 07 08 09
```

<br/>

```{r, eval = F}
# Listing file names of all the newly generated GWAS summary statistics.
file_names <- list.files(here("Output/Temp/"))[str_detect(list.files(here("Output/Temp/")), regex("rs[0-9]+_split[0-9]+_2.assoc.logistic"))]

# Reading all the new GWAS summary statistics back into R.
for(i in file_names){
  assign(i, read.table(paste0(here("Output/Temp/"), i), header = T) %>% filter(TEST == "ADD"))
}

# Combining the summary statistics for each split together to give a complete summary stat file for each SNP. Also extracting the new lead SNP and adding a column to the summary stats.
# Making temporary empty vector called tmp.
tmp <- c()

# Initiating for loop over each row of the gout_top_resid table (i.e. for each of the 3 loci).
for(i in 1:nrow(gout_top_resid)){
  
  # Making temporary empty vector called tmp3.
  tmp3 <- c()
  
  # Initiating nested for loop over the values from 0 to 9.
  for(j in 0:9){
    
    # Save summary statistic for the current SNP and current split number as tmp2.
    tmp2 <- get(paste0(gout_top_resid$RSID[i], "_split0", j, "_2.assoc.logistic"))
    
    # Concatenate each summary statistic together into tmp3.
    tmp3 <- rbind(tmp3, tmp2)
  }
  
  # Remove any rows containing NA's (i.e. those conditioned on themselves) and save as <RSID>_gwas2.
  assign(paste0(gout_top_resid$RSID[i], "_gwas2"), tmp3 %>% na.omit())
  
  # Extract the most significant SNP after conditioning along with its p-value.
  tmp3 <- tmp3 %>% select(SNP, P) %>% arrange(P) %>% slice(1)
  
  # Add the new lead SNP to the tmp table.
  tmp <- rbind(tmp, tmp3)
}

# Renaming columns.
tmp <- tmp %>% 
  rename(new_lead2 = SNP, new_p2 = P)

# Adding new lead SNP and P-value columns.
gout_top3 <- gout_top_resid %>% 
  cbind(tmp)

# Filtering to only keep SNPs that are still significant at P < 5e-8.
gout_top_resid2 <- gout_top3 %>%
  filter(new_p2 < 5e-8)

# Cleaning up environment.
rm(list = ls()[str_detect(ls(), ".assoc")], i, tmp, tmp2, file_names, tmp3, j)

# Third round of conditioning
# Making file with list of all remaining SNPs for bash script below.
write_delim(gout_top_resid2 %>% select(RSID), file = here("Output/Temp/round2_rsid.txt"), col_names = F)

# Making file with list of all previous lead SNPs for bash script below.
write_delim(gout_top_resid2 %>% select(new_lead), file = here("Output/Temp/round2_rsid2.txt"), col_names = F)

# Making file with list of all new lead SNPs for bash script below.
write_delim(gout_top_resid2 %>% select(new_lead2), file = here("Output/Temp/round2_rsid3.txt"), col_names = F)
```

<br/>

```{bash, eval = F}
# Setting directory as a variable.
PRS_DIR=/Volumes/archive/userdata/student_users/nicksumpter/Documents/PhD/PRS

# Concatenating the conditioning SNP of interest onto each split file.
parallel "echo {4} >> {1}/Output/Temp/{2}_snps_split{3}" ::: ${PRS_DIR} :::: ${PRS_DIR}/Output/Temp/round2_rsid.txt ::: 00 01 02 03 04 05 06 07 08 09 :::: ${PRS_DIR}/Output/Temp/round2_rsid3.txt

# Writing out SNP IDs into a series of files for conditioning next round of GWAS.
parallel --xapply "echo $'{2}\n{3}\n{4}' > {1}/Output/Temp/{2}_3" ::: ${PRS_DIR} :::: ${PRS_DIR}/Output/Temp/round2_rsid.txt :::: ${PRS_DIR}/Output/Temp/round2_rsid2.txt :::: ${PRS_DIR}/Output/Temp/round2_rsid3.txt

# Running third round of conditional GWAS's in parallel.
parallel "plink1.9b6.10 --bfile {1}/Output/Temp/{2} --extract {1}/Output/Temp/{2}_snps_split{3} --logistic sex --ci 0.95 --covar {1}/Data/GWAS/gout_gwas_covar.covar --covar-name Age,pc1-pc40 --condition-list {1}/Output/Temp/{2}_3 --out {1}/Output/Temp/{2}_split{3}_3" ::: ${PRS_DIR} :::: ${PRS_DIR}/Output/Temp/round2_rsid.txt ::: 00 01 02 03 04 05 06 07 08 09
```

<br/>

```{r, eval = F}
# Listing file names of all the newly generated GWAS summary statistics.
file_names <- list.files(here("Output/Temp/"))[str_detect(list.files(here("Output/Temp/")), regex("rs[0-9]+_split[0-9]+_3.assoc.logistic"))]

# Reading all the new GWAS summary statistics back into R.
for(i in file_names){
  assign(i, read.table(paste0(here("Output/Temp/"), i), header = T) %>% filter(TEST == "ADD"))
}

# Combining the summary statistics for each split together to give a complete summary stat file for each SNP. Also extracting the new lead SNP and adding a column to the summary stats.
# Making temporary empty vector called tmp.
tmp <- c()

# Making temporary empty vector called tmp3.
tmp3 <- c()

# Initiating for loop over the values from 0 to 9.
for(j in 0:9){
  
  # Save summary statistic for the current SNP and current split number as tmp2.
  tmp2 <- get(paste0(gout_top_resid2$RSID, "_split0", j, "_3.assoc.logistic"))
  
  # Concatenate each summary statistic together into tmp3.
  tmp3 <- rbind(tmp3, tmp2)
}

# Remove any rows containing NA's (i.e. those conditioned on themselves) and save as <RSID>_gwas3.
assign(paste0(gout_top_resid2$RSID, "_gwas3"), tmp3 %>% na.omit())

# Extract the most significant SNP after conditioning along with its p-value.
tmp3 <- tmp3 %>% select(SNP, P) %>% arrange(P) %>% slice(1)

# Add the new lead SNP to the tmp table.
tmp <- rbind(tmp, tmp3)

# Renaming columns.
tmp <- tmp %>% 
  rename(new_lead3 = SNP, new_p3 = P)

# Adding new lead SNP and P-value columns.
gout_top4 <- gout_top_resid2 %>% 
  cbind(tmp)

# Filtering to only keep SNPs that are still significant at P < 5e-8.
gout_top_resid3 <- gout_top4 %>%
  filter(new_p3 < 5e-8)

# Cleaning up environment.
rm(list = ls()[str_detect(ls(), ".assoc")], i, tmp, tmp2, file_names, tmp3, j)



# Locus zooms ---------------------------------------
# Loading in code and gene list. This was cloned from the https://github.com/Geeketics/LocusZooms repository.
source(here("Script/Functions/locus_zoom.R"))

# Loading in the UCSC gene list for annotating the locus zooms.
UCSC_GRCh37_Genes_UniqueList.txt <- read.delim(here("Data/GWAS/UCSC_GRCh37_Genes_UniqueList.txt"))
```

<br/>

```{bash, eval = F}
# Setting directory as a variable.
PRS_DIR=/Volumes/archive/userdata/student_users/nicksumpter/Documents/PhD/PRS

# Calculating LD for each original GWAS lead SNP vs all other SNPs in that chromosome.
parallel --xapply "plink1.9b6.10 --bfile {3}/Output/Temp/chr{1}_tmp --r2 inter-chr --ld-snp {2} --ld-window-r2 0 --out {3}/Output/Temp/chr{1}_{2}_ld" :::: ${PRS_DIR}/Output/Temp/all_chr.txt :::: ${PRS_DIR}/Output/Temp/all_rsid.txt ::: ${PRS_DIR}
```

<br/>

```{r, eval = F}
# Reading the LD reports back into R.
for(i in 1:nrow(gout_top)){
  assign(paste0("chr", gout_top$CHR[i], "_", gout_top$RSID[i], "_ld"), read_table(paste0(here("Output/Temp/"), "chr", gout_top$CHR[i], "_", gout_top$RSID[i], "_ld.ld")))
}

# Extracting new lead SNP IDs from first round of conditioning.
first_round <- gout_top_resid %>% 
  select(new_lead) %>% 
  rename(RSID = new_lead)

# Extracting new lead SNP IDs from second round of conditioning.
second_round <- gout_top_resid2 %>% 
  select(new_lead2) %>% 
  rename(RSID = new_lead2)

# Making full list of lead SNPs (including conditionally associated SNPs) and re-adding metadata.
gout_top_full <- gout_top %>% 
  select(RSID) %>% 
  rbind(first_round, second_round) %>% 
  left_join(loci, by = "RSID") %>% 
  arrange(CHR, BP)

# Plotting the locus zooms of the initial unconditioned GWAS.
# For each SNP in gout_top (15 SNPs total).
for(i in 1:nrow(gout_top)){
  
  # Make temporary data file for locus zoom function.
  tmp <- loci %>% 
    mutate(SNP = RSID) %>% 
    filter(!is.na(SNP), CHR == gout_top$CHR[i] & between(BP, gout_top$BP1[i], gout_top$BP2[i]))
  
  # Plot locus zoom for the entirety of each locus, with no offset, using the ld files we just created, using the gene list we read in, naming the plot "Unconditioned <RSID> Locus Zoom", saving the plot as a jpg file of the form "Chr<chrnum>_<bp1>_<bp2>_<rsid>_unconditioned.jpg", also labeling all lead SNPs at that locus.
  locus.zoom(data = tmp,
             region = c(gout_top$CHR[i], gout_top$BP1[i], gout_top$BP2[i]),
             offset_bp = 0,
             ld.file = get(paste0("chr", gout_top$CHR[i], "_", gout_top$RSID[i], "_ld")),
             genes.data = UCSC_GRCh37_Genes_UniqueList.txt,
             plot.title = paste0("Unconditioned ", gout_top$RSID[i], " Locus Zoom"),
             file.name = paste0(here("Output/Plots/"), "Chr", gout_top$CHR[i], "_", gout_top$BP1[i], "_", gout_top$BP2[i], "_", gout_top$RSID[i], "_unconditioned", ".jpg"),
             secondary.snp = gout_top_full$RSID,
             secondary.label = TRUE)
}

# Plotting the locus zooms of the first round of conditioning.
# Making file with list of all SNPs in new_lead column for bash script below.
write_delim(gout_top2 %>% select(new_lead), file = here("Output/Temp/round1_all_rsid.txt"), col_names = F)
```

<br/>

```{bash, eval = F}
# Setting directory as a variable.
PRS_DIR=/Volumes/archive/userdata/student_users/nicksumpter/Documents/PhD/PRS

# Calculating LD for each newly conditioned GWAS lead SNP vs all other SNPs in that chromosome.
parallel --xapply "plink1.9b6.10 --bfile {3}/Output/Temp/chr{1}_tmp --r2 inter-chr --ld-snp {2} --ld-window-r2 0 --out {3}/Output/Temp/chr{1}_{2}_ld" :::: ${PRS_DIR}/Output/Temp/all_chr.txt :::: ${PRS_DIR}/Output/Temp/round1_all_rsid.txt ::: ${PRS_DIR}
```

<br/>

```{r, eval = F}
# Reading the LD reports back into R.
for(i in 1:nrow(gout_top2)){
  assign(paste0("chr", gout_top2$CHR[i], "_", gout_top2$new_lead[i], "_ld"), read_table(paste0(here("Output/Temp/"), "chr", gout_top2$CHR[i], "_", gout_top2$new_lead[i], "_ld.ld")))
}

# For each SNP in gout_top2 (15 SNPs total).
for(i in 1:nrow(gout_top2)){
 # Plot locus zoom for the entirety of each locus, with no offset, using the ld files we just created, using the gene list we read in, naming the plot "Conditioned on <RSID>", saving the plot as a jpg file of the form "Chr<chrnum>_<bp1>_<bp2>_<rsid>_condition_<rsid>.jpg", also labeling all lead SNPs at that locus.
  locus.zoom(data = get(paste0(gout_top2$RSID[i], "_gwas")),
             region = c(gout_top$CHR[i], gout_top2$BP1[i], gout_top2$BP2[i]),
             offset_bp = 0,
             ld.file = get(paste0("chr", gout_top2$CHR[i], "_", gout_top2$new_lead[i], "_ld")),
             genes.data = UCSC_GRCh37_Genes_UniqueList.txt,
             plot.title = paste0("Conditioned on ", gout_top2$RSID[i]),
             file.name = paste0(here("Output/Plots/"), "Chr", gout_top2$CHR[i], "_", gout_top2$BP1[i], "_", gout_top2$BP2[i], "_", gout_top2$RSID[i], "_condition_", gout_top2$RSID[i], ".jpg"),
             secondary.snp = gout_top_full$RSID,
             secondary.label = TRUE)
}

# Plotting locus zooms of second round of conditioning.
# Making file with list of all SNPs in CHR column for bash script below.
write_delim(gout_top3 %>% select(CHR), file = here("Output/Temp/round2_all_chr.txt"), col_names = F)

# Making file with list of all SNPs in new_lead column for bash script below.
write_delim(gout_top3 %>% select(new_lead2), file = here("Output/Temp/round2_all_rsid.txt"), col_names = F)
```

<br/>

```{bash, eval = F}
# Setting directory as a variable.
PRS_DIR=/Volumes/archive/userdata/student_users/nicksumpter/Documents/PhD/PRS

# Calculating LD for each newly conditioned GWAS lead SNP vs all other SNPs in that chromosome.
parallel --xapply "plink1.9b6.10 --bfile {3}/Output/Temp/chr{1}_tmp --r2 inter-chr --ld-snp {2} --ld-window-r2 0 --out {3}/Output/Temp/chr{1}_{2}_ld" :::: ${PRS_DIR}/Output/Temp/round1_all_chr.txt :::: ${PRS_DIR}/Output/Temp/round2_all_rsid.txt ::: ${PRS_DIR}
```

<br/>

```{r, eval = F}
# Reading the LD reports back into R.
for(i in 1:nrow(gout_top3)){
  assign(paste0("chr", gout_top3$CHR[i], "_", gout_top3$new_lead2[i], "_ld"), read_table(paste0(here("Output/Temp/"), "chr", gout_top3$CHR[i], "_", gout_top3$new_lead2[i], "_ld.ld")))
}

# For each SNP in gout_top3 (3 SNPs total).
for(i in 1:nrow(gout_top3)){
  # Plot locus zoom for the entirety of each locus, with no offset, using the ld files we just created, using the gene list we read in, naming the plot "Conditioned on <RSID> and <RSID>", saving the plot as a jpg file of the form "Chr<chrnum>_<bp1>_<bp2>_<rsid>_condition_<rsid>and<rsid>.jpg", also labeling all lead SNPs at that locus.
  locus.zoom(data = get(paste0(gout_top3$RSID[i], "_gwas2")),
             region = c(gout_top3$CHR[i], gout_top3$BP1[i], gout_top3$BP2[i]),
             offset_bp = 0,
             ld.file = get(paste0("chr", gout_top3$CHR[i], "_", gout_top3$new_lead2[i], "_ld")),
             genes.data = UCSC_GRCh37_Genes_UniqueList.txt,
             plot.title = paste0("Conditioned on ", gout_top3$RSID[i], " and ", gout_top3$new_lead[i]),
             file.name = paste0(here("Output/Plots/"), "Chr", gout_top3$CHR[i], "_", gout_top3$BP1[i], "_", gout_top3$BP2[i], "_", gout_top3$RSID[i], "_condition_", gout_top3$RSID[i], "and", gout_top3$new_lead[i], ".jpg"),
             secondary.snp = gout_top_full$RSID,
             secondary.label = TRUE)
}


# Plotting locus zooms of third round of conditioning.
# Making file with list of all SNPs in CHR column for bash script below.
write_delim(gout_top4 %>% select(CHR), file = here("Output/Temp/round3_all_chr.txt"), col_names = F)

# Making file with list of all SNPs in new_lead column for bash script below.
write_delim(gout_top4 %>% select(new_lead3), file = here("Output/Temp/round3_all_rsid.txt"), col_names = F)
```

<br/>

```{bash, eval = F}
# Setting directory as a variable.
PRS_DIR=/Volumes/archive/userdata/student_users/nicksumpter/Documents/PhD/PRS

# Calculating LD for each newly conditioned GWAS lead SNP vs all other SNPs in that chromosome.
parallel --xapply "plink1.9b6.10 --bfile {3}/Output/Temp/chr{1}_tmp --r2 inter-chr --ld-snp {2} --ld-window-r2 0 --out {3}/Output/Temp/chr{1}_{2}_ld" :::: ${PRS_DIR}/Output/Temp/round3_all_chr.txt :::: ${PRS_DIR}/Output/Temp/round3_all_rsid.txt ::: ${PRS_DIR}
```

<br/>

```{r, eval = F}
# Reading the LD reports back into R.
for(i in 1:nrow(gout_top4)){
  assign(paste0("chr", gout_top4$CHR[i], "_", gout_top4$new_lead3[i], "_ld"), read_table(paste0(here("Output/Temp/"), "chr", gout_top4$CHR[i], "_", gout_top4$new_lead3[i], "_ld.ld")))
}

# Plotting locus zooms
# For each SNP in gout_top4 (1 SNP total).
for(i in 1:nrow(gout_top4)){
  # Plot locus zoom for the entirety of the locus, with no offset, using the ld file we just created, using the gene list we read in, naming the plot "Conditioned on <RSID> and <RSID> and <RSID>", saving the plot as a jpg file of the form "Chr<chrnum>_<bp1>_<bp2>_<rsid>_condition_<rsid>and<rsid>and<rsid>.jpg", also labeling all lead SNPs at that locus.
  locus.zoom(data = get(paste0(gout_top4$RSID[i], "_gwas3")),
             region = c(gout_top4$CHR[i], gout_top4$BP1[i], gout_top4$BP2[i]),
             offset_bp = 0,
             ld.file = get(paste0("chr", gout_top4$CHR[i], "_", gout_top4$new_lead3[i], "_ld")),
             genes.data = UCSC_GRCh37_Genes_UniqueList.txt,
             plot.title = paste0("Conditioned on ", gout_top4$RSID[i], " and ", gout_top4$new_lead[i], " and ", gout_top4$new_lead2[i]),
             file.name = paste0(here("Output/Plots/"), "Chr", gout_top4$CHR[i], "_", gout_top4$BP1[i], "_", gout_top4$BP2[i], "_", gout_top4$RSID[i], "_condition_", gout_top4$RSID[i], "and", gout_top4$new_lead[i], "and", gout_top4$new_lead2[i], ".jpg"),
             secondary.snp = gout_top_full$RSID,
             secondary.label = TRUE)
}



# Combining all GWAS results together into final list. ----------------------------------------------
# Extracting regions of all loci.
regions <- gout_top %>%
  select(CHR, BP1, BP2)

# Making empty vector named out.
out <- c()

# For each row in regions (i.e. each of 15 loci).
for(i in 1:nrow(regions)){
  
  # Adding the start and end coordinates of the corresponding region.
  tmp <- gout_top_full %>% 
    filter(CHR == regions$CHR[i] & between(BP, regions$BP1[i], regions$BP2[i])) %>% 
    mutate(BP1 = regions$BP1[i], 
           BP2 = regions$BP2[i])
  
  # Adding to the out vector.
  out <- rbind(out, tmp)
}

# Save out as gout_top_full.
gout_top_full <- out

# For each of the loci with multiple SNPs, testing the association of each SNP after adjusting for all others at the locus.
# Need to write it such that it asks: for every locus with more than one SNP after conditioning, run an association test for each SNP conditioned on all other SNPs at that locus.
# Extracting all SNPs from those loci.
for(i in unique(gout_top_full$BP1[duplicated(gout_top_full$BP1)])){
  
  assign(paste0(i, "_locus"), gout_top_full %>% filter(BP1 == i))
  
  for(j in 1:NROW(get(paste0(i, "_locus")))){
    tmp <- get(paste0(i, "_locus")) %>% 
      select(RSID) %>% 
      slice(-j)
  
    write_delim(tmp, file = paste0(here("Output/Temp/"), "locus_", i, "_snps_", j, ".txt"), delim = "\n", col_names = F)
  }
  
  
  
  tmp <- get(paste0(i, "_locus")) %>% 
    select(CHR)
  
  write_delim(tmp, file = paste0(here("Output/Temp/"), "locus_", i, "_chr.txt"), delim = "\n", col_names = F)
}

# Write out another file called multi_chr.txt with both chromosomes in it.
write_delim(unique(multi_snps %>% select(CHR)), file = here("Output/Temp/multi_chr.txt"), delim = "\n", col_names = F)


```

<br/>

```{bash, eval = F}
# Setting directory as a variable.
PRS_DIR=/Volumes/archive/userdata/student_users/nicksumpter/Documents/PhD/PRS

# Extracting SNPs of interest for each chromosome of interest.
parallel "plink1.9b6.10 --bfile {2}/Output/Temp/chr{1}_tmp --extract {2}/Output/Temp/snps_to_extract.txt --make-bed --out {2}/Output/Temp/chr{1}_test" :::: ${PRS_DIR}/Output/Temp/multi_chr.txt ::: ${PRS_DIR}

# 
plink1.9b6.10 --bfile ${PRS_DIR}/Output/Temp/chr4_test --bmerge ${PRS_DIR}/Output/Temp/chr11_test --make-bed --out ${PRS_DIR}/Output/Temp/merged_test

while IFS= read -r RSID; do
plink1.9b6.10 --bfile ${PRS_DIR}/Output/Temp/merged_test --logistic sex --ci 0.95 --covar ${PRS_DIR}/Data/GWAS/gout_gwas_covar.covar --covar-name Age,pc1-pc40 --condition ${RSID} --out ${PRS_DIR}/Output/Temp/final_gwas_${RSID}
done < ${PRS_DIR}/Output/Temp/snps_to_extract.txt
```

<br/>

```{r, eval = F}


for(i in 4:7){
  system(paste0('source ~/.bashrc; plink1.9b6.10 --bfile ', here("Output/Temp/"), 'merged_test --logistic sex --ci 0.95 --covar ', here("Data/GWAS", "gout_gwas_covar.covar"), ' --covar-name Age,pc1-pc40 --condition ', multi_snps$RSID[i], ' --out ', here("Output/Temp/"), 'final_gwas_', multi_snps$RSID[i]))
}

for(i in 1:3){
  system(paste0('source ~/.bashrc; plink1.9b6.10 --bfile ', here("Output/Temp/"), 'merged_test --logistic sex --ci 0.95 --covar ', here("Data/GWAS", "gout_gwas_covar.covar"), ' --covar-name Age,pc1-pc40 --condition-list ', here("Output/Temp/"), 'conditionlist_', multi_snps$RSID[i], '.txt --out ', here("Output/Temp/"), 'final_gwas_', multi_snps$RSID[i]))
}

file_names <- list.files(here("Output/Temp/"))[str_detect(list.files(here("Output/Temp/")), "final_gwas_.+logistic")]

for(i in file_names){
  assign(i, read.table(paste0(here("Output/Temp/"), i), header = T) %>% filter(TEST == "ADD"))
}

tmp <- c()
for(i in 1:3){
  tmp2 <- get(paste0("final_gwas_", multi_snps$RSID[i], ".assoc.logistic")) %>% slice(i)
  tmp <- rbind(tmp, tmp2)
}

tmp2 <- c()
for(i in 4:7){
  tmp3 <- get(paste0("final_gwas_", multi_snps$RSID[i], ".assoc.logistic")) %>% slice(-(1:3))
  tmp2 <- rbind(tmp2, tmp3)
}

tmp2 <- tmp2 %>% slice(5, 2, 15, 12)

multi_snps2 <- rbind(tmp, tmp2)

tmp2 <- multi_snps2 %>% 
  select(CHR:BP, OR:U95, P)

multi_snps3 <- left_join(multi_snps, tmp2, by = c("CHR", "BP"))

system(paste0('source ~/.bashrc; plink1.9b6.10 --bfile ', here("Output/Temp/"), 'merged_test --r2 inter-chr --ld-window-r2 0 --out ', here("Output/Temp/"), 'merged_ld'))

merged_ld <- read_table(paste0(here("Output/Temp/"), "merged_ld.ld"))

# test multicollinearity in SLC2A9 model in next Rmd

single_snps <- gout_top_full %>% 
  filter(!(BP1 %in% names(table(gout_top_full$BP1)[table(gout_top_full$BP1) > 1])))

single_snps2 <- single_snps %>% 
  select(CHR, RSID, BP:Alternate_Allele, OR:U95, P, EAF, INFO, BP1, BP2)

multi_snps4 <- multi_snps3 %>% 
  select(CHR, RSID, BP:Alternate_Allele, OR.y:U95.y, P.y, OR.x:U95.x, P.x, EAF, INFO, BP1, BP2) %>% 
  rename(OR = OR.y,
         SE = SE.y, 
         L95 = L95.y,
         U95 = U95.y,
         P = P.y,
         OR_old = OR.x,
         SE_old = SE.x, 
         L95_old = L95.x,
         U95_old = U95.x,
         P_old = P.x)

gout_top_final <- full_join(multi_snps4, single_snps2) %>% 
  arrange(CHR, BP)

# Flipping allele order + OR etc so effect allele is always the gout risk allele + labelling based on locus zooms
smallOR <- gout_top_final %>% 
  filter(OR < 1) %>% 
  mutate(OR = as.numeric(signif(1/OR, digits = 4)),
         tmp_L = as.numeric(signif(1/L95, digits = 4)),
         tmp_U = as.numeric(signif(1/U95, digits = 4)),
         U95 = tmp_L,
         L95 = tmp_U,
         OR_old = as.numeric(round(1/OR_old, digits = 3)),
         tmp_L_old = as.numeric(round(1/L95_old, digits = 3)),
         tmp_U_old = as.numeric(round(1/U95_old, digits = 3)),
         U95_old = tmp_L_old,
         L95_old = tmp_U_old,
         EAF = 1 - EAF) %>% 
  rename(allele2 = Effect_Allele,
         allele1 = Alternate_Allele) %>% 
  rename(Alternate_Allele = allele2,
         Effect_Allele = allele1) %>% 
  select(CHR:BP, Effect_Allele, Alternate_Allele, OR:BP2)
bigOR <- gout_top_final %>% 
  filter(OR > 1)
gout_top_final <- full_join(smallOR, bigOR) %>% 
  arrange(CHR, BP) %>% 
  mutate(Locus_Name = c("PDZK1", "TRIM46", "GCKR", "SFMBT1", "SLC2A9", "SLC2A9", "SLC2A9", "ABCG2", "ABCG2", "SLC17A1", "ZSCAN31", "MLXIPL", "SLC16A9", "SLC22A11", "SLC22A11", "OVOL1", "R3HDM2", "MLXIP", "PNPLA3"))

UKBB_Gene_OR <- gout_top_final
  
save(UKBB_Gene_OR, file = here("Output/UKBB_Gene_OR.RData"))

# Cleaning up
rm(list = ls()[str_detect(ls(), "^chr|^gout_top|^final_|gwas$")], bigOR, first_round, multi_snps, multi_snps2, multi_snps3, multi_snps4, merged_ld, out, regions, second_round, single_snps, smallOR, third_round, tmp, tmp2, tmp3, file_names, i, sumstat_signif, single_snps2, loci, sumstat_final, UCSC_GRCh37_Genes_UniqueList.txt, check.rsid, elog10, gene.position, get.ld, get.region, locus.zoom, merge.gene.colour, merge.plot.dat, plot.locus, plot.secondary.point, read.plink.loci, round.up, subset.data)
```

<br/>

### Locus-Zooms {.tabset}

All of the locus zooms are plotted below in separate tabs:

```{r Plots_tables, warning = F, message = F}
file_names <- list.files(here("Output/Plots"), full.names = T)[str_detect(list.files(here("Output/Plots"), full.names = T), "Chr")]

tmp <- file_names %>% 
  as_tibble() %>% 
  separate(value, sep = "_", into = c(NA, "X2", "BP1", NA, NA, "Cond", "CondSNPs")) %>% 
  rownames_to_column() %>% 
  mutate(Cond1 = case_when(Cond == "unconditioned.jpg" ~ FALSE, TRUE ~ TRUE), 
         BP1 = as.numeric(BP1), 
         rowname = as.numeric(rowname)) %>% 
  separate(X2, sep = "/", into = c(NA, NA, NA, NA, NA, NA, NA, "CHR")) %>%
  mutate(CHR = as.numeric(str_replace(CHR, "Chr", ""))) %>% 
  arrange(CHR, BP1, Cond1, CondSNPs)

tmp1 <- tmp %>% 
  pull(BP1) %>% 
  unique() %>% 
  as_tibble() %>% 
  rename(BP1 = value)

load(here("Output/UKBB_Gene_OR.RData"))

tmp2 <- UKBB_Gene_OR %>% 
  pull(Locus_Name) %>% 
  unique() %>% 
  as_tibble() %>% 
  rename(Locus_Name = value) %>% 
  cbind(tmp1)

tmp3 <- tmp %>% 
  left_join(tmp2, by = "BP1")

tmp4 <- tmp3 %>% 
  mutate(CondSNPs2 = str_remove(CondSNPs, ".jpg")) %>% 
  separate(CondSNPs2, sep = "and", into = c("SNP1", "SNP2", "SNP3", "SNP4")) %>% 
  mutate(SNPs = case_when(is.na(SNP2) ~ SNP1,
                          !is.na(SNP2) & is.na(SNP3) ~ str_c(SNP1, SNP2, sep = " and "),
                          !is.na(SNP3) & is.na(SNP4) ~ str_c(SNP1, SNP2, SNP3, sep = " and "),
                          !is.na(SNP4) ~ str_c(SNP1, SNP2, SNP3, SNP4, sep = " and ")),
         Plot_Name = case_when(!Cond1 ~ paste0(Locus_Name, " (Uncond.)"),
                               Cond1 ~ paste0(Locus_Name, " (Cond. on ", SNPs, ")")))

file_names2 <- file_names[tmp$rowname]

names(file_names2) <- tmp4$Plot_Name

template <- c(
    "#### {{nm}}\n",
    "```{r, echo = FALSE}\n",
    "include_graphics(file_names2['{{nm}}'])\n",
    "```\n",
    "\n"
  )

plots <- lapply(
  tmp4$Plot_Name, 
  function(nm) knit_expand(text = template)
)
```

`r knitr::knit(text = unlist(plots))`

### Summary

In summary, I produced a list of SNPs that will be used to create a PRS. This uses SNPs genotyped on the Human CoreExome v1.0 chip. This will ensure that no imputation was done on Polynesian individuals, which should make the genotypes more reliable. Three of the 15 total loci had more than one partially independent genome-wide significant signal. These were at SLC2A9 (3 hits), ABCG2 (2 hits), and SLC22A11 (2 hits). 

### Tin Urate PRS

Based on comments from reviewers, it was requested that I produce a PRS based on the results of Tin et al., 2019. The following code details the following:

1. CoreExome genotyped plink files were filtered to exclude variants with over 10% missingness and those with MAF less than 0.01 in the entire CoreExome cohort

2. The locations of these filtered SNPs were extracted from the bim file and this was filtered to only include SNPs that were also in the UK Biobank imputed genotype list

3. Finally, the Tin et al. European summary statistics were filtered to only keep SNPs matching the chromosome and location of the above SNPs

```{bash Tin_bash, engine.opts = '-l', eval = F}
mkdir -p /Volumes/scratch/merrimanlab/Nick/PRS

cd /Volumes/scratch/merrimanlab/Nick/PRS

# Filter tin sumstats to match both chromosome and bp of CoreEx SNPs plus remove rare SNPs (less than 1% frequency)
plink1.9b6.10 --bfile /Volumes/archive/merrimanlab/raid_backup/New_Zealand_Chip_data/CoreExome/QC_MergedBatches/Final_Data/CZ-MB1.2-QC1.10_CoreExome24-1.0-3_genotyped-QCd_rsIDconverted --make-bed --geno 0.1 --maf 0.01 --out filtered_coreex

cut -f1,4 filtered_coreex.bim | tr '\t' ' ' > coreex_snps.txt

for i in {1..22}
do 
  grep -w $i coreex_snps.txt | cut -d ' ' -f2 > snplist_chr$i.txt
done

# Now filter for SNPs in the UK Biobank imputed genotype list
parallel "cut -f3 /Volumes/scratch/merrimanlab/ukbio/EGAD00010001474/splits/ukb_mfi_chr{}_v3.txt | grep -Fwf snplist_chr{}.txt | sed 's/^/{} /' > in_ukb_chr{}.txt" ::: {1..22}

cat $(ls | grep in_ukb) > snplist.txt

cat <(head -n1 /Volumes/archive/merrimanlab/central_datasets/summary_gwas/urate/Tin_2019/cleaned/urate_chr1_22_LQ_IQ06_mac10_EA_60_rsid.txt) <(awk '$6 < 0.99 && $6 > 0.01' FS=' ' /Volumes/archive/merrimanlab/central_datasets/summary_gwas/urate/Tin_2019/cleaned/urate_chr1_22_LQ_IQ06_mac10_EA_60_rsid.txt | grep -Ff snplist.txt) | sed 's/ /\t/g' > tin_filtered.txt
```

4. The filtered Tin summary statistics were read back into R, then further cleaned up to ensure no rare variants remained

5. The summary statistics were used to define a list of top SNPs in a similar manner to the previous code for the UK Biobank gout GWAS

6. No conditional analyses were run on this list of summary statistics

7. In total, 82 common variants were defined as being associated with serum urate in Europeans, all of which were suitable for downstream analysis

```{r Tin_R, eval = F}
tin <- vroom("/Volumes/scratch/merrimanlab/Nick/PRS/tin_filtered.txt", 
             delim = "\t", 
             col_names = T)

test <- tin %>% 
  filter(str_detect(RSID, regex("^rs[0-9]+")))

test2 <- tin %>% 
  filter(!(RSID %in% test$RSID)) %>% 
  mutate(RSID = paste0(Chr, Pos_b37, Allele1, Allele2))

tin2 <- rbind(test, test2) %>% 
  arrange(Chr, Pos_b37) %>% 
  rename(CHR = Chr,
         BP = Pos_b37,
         P = `P-value`,
         Beta = Effect,
         SE = StdErr,
         EAF = Freq1,
         Effect_Allele = Allele1,
         Alternate_Allele = Allele2) %>% 
  mutate(t = abs(Beta/SE))

# nrow(tin2 %>% select(CHR, BP) %>% unique()) # all unique

sumstat_signif <- tin2 %>% 
  filter(P <= 5e-8,
         EAF > 0.01,
         EAF < 0.99) %>% 
  arrange(desc(t))

# Grouping into loci +- 500 kb of top SNPs
gout_top <- sumstat_signif %>% 
  slice(1)
gout2 <- sumstat_signif %>% 
  filter(!(CHR == gout_top$CHR[1] & BP %in% ((gout_top$BP[1] - 500000):(gout_top$BP[1] + 500000))))

while(nrow(gout2) > 0) {
  tmp <- gout2 %>% 
    slice(1)
  gout_top <- rbind(tmp, gout_top)
  gout2 <- gout2 %>% 
    filter(!(CHR == gout_top$CHR[1] & BP %in% ((gout_top$BP[1] - 500000):(gout_top$BP[1] + 500000))))
} 

gout_top <- gout_top %>% 
  arrange(CHR, BP)


# Finding regions of loci
sumstat_signif <- sumstat_signif %>% 
  arrange(CHR, BP)

out <- NA
for(i in 2:nrow(sumstat_signif)) {
  if(sumstat_signif$CHR[i] == sumstat_signif$CHR[i - 1]){
    out[i] <- sumstat_signif$BP[i] - sumstat_signif$BP[i - 1]
  } else {
    out[i] <- NA
  }
}

tmp <- sumstat_signif %>% 
  mutate(Diff = out,
         Diff2 = case_when(Diff < 500000 ~ Diff))

out <- sumstat_signif %>% slice(1)
for(i in 2:nrow(sumstat_signif)) {
  if(is.na(tmp$Diff2[i])){
    out <- rbind(out, sumstat_signif %>% slice(i - 1), sumstat_signif %>% slice(i))
  }
}
out <- rbind(out, sumstat_signif %>% slice(nrow(sumstat_signif)))

# Extracting regions
bgen_ranges <- out %>% select(CHR, BP)

tmp1 <- bgen_ranges %>% slice(seq(1, nrow(bgen_ranges), by = 2)) %>% rename(BP1 = BP)

tmp2 <- bgen_ranges %>% slice(seq(2, nrow(bgen_ranges), by = 2)) %>% rename(CHR.x = CHR, BP2 = BP)

bgen_ranges <- tmp1 %>% 
  cbind(tmp2) %>% 
  mutate(BP1 = BP1 - 50000,
         BP2 = BP2 + 50000) %>% 
  select(-CHR.x)

out <- c()
for(i in 1:nrow(bgen_ranges)){
  tmp <- gout_top %>% 
    filter(CHR == bgen_ranges$CHR[i] & between(BP, bgen_ranges$BP1[i], bgen_ranges$BP2[i])) %>% 
    arrange(P) %>% 
    slice(1)
  out <- rbind(out, tmp)
}

gout_top <- out %>% 
  cbind(bgen_ranges %>% select(-CHR))

rm(tmp, bgen_ranges, out, i)

# Flipping allele order + OR etc so effect allele is always the gout risk allele + labelling based on locus zooms
gout_top <- gout_top %>% 
  mutate(L95 = Beta - 1.96 * SE,
         U95 = Beta + 1.96 * SE)

smallOR <- gout_top %>% 
  filter(Beta < 0) %>% 
  mutate(Beta = as.numeric(signif(Beta * -1, digits = 4)),
         tmp_L = as.numeric(signif(L95 * -1, digits = 4)),
         tmp_U = as.numeric(signif(U95 * -1, digits = 4)),
         U95 = tmp_L,
         L95 = tmp_U,
         EAF = 1 - EAF) %>% 
  rename(allele2 = Effect_Allele,
         allele1 = Alternate_Allele) %>% 
  rename(Alternate_Allele = allele2,
         Effect_Allele = allele1) %>% 
  select(CHR:BP, RSID, Effect_Allele, Alternate_Allele, Beta, L95, U95, SE:BP2)
bigOR <- gout_top %>% 
  filter(Beta > 0) %>% 
  mutate(Beta = as.numeric(signif(Beta, digits = 4)),
         L95 = as.numeric(signif(L95, digits = 4)),
         U95 = as.numeric(signif(U95, digits = 4))) %>% 
  select(CHR:BP, RSID, Effect_Allele, Alternate_Allele, Beta, L95, U95, SE:BP2)

gout_top_final <- full_join(smallOR, bigOR) %>% 
  arrange(CHR, BP) %>% 
  select(-t, -n_total_sum) %>% 
  mutate(Effect_Allele = toupper(Effect_Allele),
         Alternate_Allele = toupper(Alternate_Allele)) #%>% 
  #mutate(Locus_Name = c("PDZK1", "TRIM46", "GCKR", "SFMBT1", "SLC2A9", "SLC2A9", "SLC2A9", "ABCG2", "ABCG2", "SLC17A1", "ZSCAN31", "MLXIPL", "SLC16A9", "SLC22A11", "SLC22A11", "OVOL1", "R3HDM2", "MLXIP", "PNPLA3"))

Tin_Gene_OR <- gout_top_final
  
save(Tin_Gene_OR, file = here("Output/Tin_Gene_OR.RData"))

# Cleaning up
rm(list = ls()[str_detect(ls(), "^chr|^gout_top|^final_|gwas$")], bigOR, smallOR, tmp1, tmp2, sumstat_signif, gout2, test, test2, tin, tin2)
```

